{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy\n",
    "* consider a binary classification problem\n",
    "    * consider 3 features $f_1, f_2 and f_3$\n",
    "    * ID-3 algorithm is to be used, so that a proper attribute can be chosen for a split to be caused at each node, w.r.t. a feature $f_i$\n",
    "    * entropy is used for this selection process\n",
    "* measures the purity of the split\n",
    "* aim of a decision tree algorithm is to arrive at the leaf node as quickly as possible\n",
    "    * at leaf node, *pure split* is guaranteed, i.e. either all samples split into class 1(they answer YES at the split site) or into class 0, **but not both**.\n",
    "* $\\textrm{H(s)} = -\\textrm{p}_{\\textrm{+}}\\textrm{log(p}_{\\textrm{+}}) -\\textrm{p}_{\\textrm{-}}\\textrm{log(p}_{\\textrm{-}}) $, + = positive class, - = negative class(this is obviosuly for binary classification), log to the *base 2*.\n",
    "    * $\\textrm{p}_{\\textrm{+}}$ = % of positive samples, $\\textrm{p}_{\\textrm{-}}$ = % of negative samples\n",
    "    * s = subset of the entire training dataset, except at the root node, where either the entire mini-batch or the entire dataset is to be split.\n",
    "    * <img src=\"dtree-1.png\" />\n",
    "    * for instance, we want to calculate entropy at node labelled $f_1$:\\\n",
    "    $\\textrm{p}_{\\textrm{+}}$ = $\\frac{9}{9+5}$, $\\textrm{p}_{\\textrm{-}}$ = $\\frac{5}{9+5}$\n",
    "    $\\textrm{H(s)} = -\\textrm{p}_{\\textrm{+}}\\textrm{log(p}_{\\textrm{+}}) -\\textrm{p}_{\\textrm{-}}\\textrm{log(p}_{\\textrm{-}})$ = $-\\frac{9}{14}\\textrm{log}_2\\left(\\frac{9}{14}\\right) - \\frac{5}{14}\\textrm{log}_2\\left(\\frac{5}{14}\\right) $ = 0.94\n",
    "    * worst split occurs when the training samples are split equally among the positive and negative, i.e. H(s) = -(1/2)log(1/2)-(1/2)log(1/2) = -log(1/2) = log(2) = 1(**since the base is 2**)\n",
    "    * best split = pure split, i.e. splitting at the leaf nodes, let there be m samples, H(s) = -(m/m)log(m/m)-0.log(0), the $2^{\\textrm{nd}}$ term is indeterminate, hence we need to solve it using limits, which results in the function xlog(x), x$\\rightarrow$0, and the answer of this limit is 0\\\n",
    "    hence H(s) = 0\n",
    "* after splitting at a non-leaf node, we may not immediately reach the leaf nodes, hence may require more intermediate splitting\n",
    "    * for this intermediate splitting to occur, we may require certain attributes\n",
    "    * hence the entropy at these intermediate parent nodes have to be also taken into account\n",
    "    * this is done via **information gain**\n",
    "* Pure means that the node contains sample from a single class, and impure means that the nodes contains samples from different classes.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information gain\n",
    "\n",
    "* gain(S, A) = H(s) - $\\sum\\limits_{\\textrm{v}\\epsilon \\textrm{VAL}}\\frac{|\\textrm{S}_{\\textrm{v}}|}{|\\textrm{S}|}\\textrm{H(S}_{\\textrm{v)}}$\n",
    "\n",
    "    * this is calculated at the root node of the decision tree\n",
    "    \n",
    "    * A - feature used to split at the current node\n",
    "\n",
    "* for the above example, gain(S, A) = 0.94 - $\\left[\\frac{5}{14}\\left(-\\frac{3}{5}\\textrm{log}_2\\left(\\frac{3}{5}\\right) -\\frac{2}{5}\\textrm{log}_2\\left(\\frac{2}{5}\\right) \\right) + \n",
    "\\frac{9}{14}\\left(-\\frac{3}{9}\\textrm{log}_2\\left(\\frac{3}{9}\\right) -\\frac{6}{9}\\textrm{log}_2\\left(\\frac{6}{9}\\right) \\right)\\right]$ = 0.94 - 0.35 - 0.18 = 0.41\\\n",
    "  leaves will have H(s) = 0, hence it doesn't matter whether they are considered for calculating gain or not.\n",
    "  \n",
    "* select the feature that yields the maximum information gain value, for splitting at the current node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gini impurity\n",
    "\n",
    "1. G.I. = 1 - $\\sum\\limits_{\\textrm{i=1}}^{\\textrm{n}}\\textrm{p}^2$, where n = total number of classes\n",
    "\n",
    "2. this is to be calculated at a splitting node, similar to entropy calculation\n",
    "\n",
    "3. at best split, entropy = 0, and G.I. = 1 - ($1^2+0^2$) = 0\n",
    "\n",
    "4. at worst split, entropy = 1, and G.I. = 1 - (1/4 + 1/4) = 0.5\n",
    "\n",
    "5. this is used, in lieu of entropy, in models such as RandomForest and other ensemble techniques, since **it is computationally more efficient**\n",
    "\n",
    "    1. in calculating entropy, apart from the multiplications, additions and subtraction operations, we also have the log function to be computed\n",
    "    \n",
    "    2. whereas in Gini impurity, only division, addition and subtraction are the main operations involved.\n",
    "    \n",
    "6. <font color=\"red\">Search for information gain as a function of G.I.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ID3-iterative dichotomiser\n",
    "\n",
    "* algorithm repeatedly divides the dataset(or a subset of the training-data) into 2 or more groups at each step\n",
    "\n",
    "* ID-3 steps:\n",
    "    1. Calculate the Information Gain of each feature.\n",
    "    \n",
    "    2. Considering that all rows don’t belong to the same class(i.e. the current node is not a leaf node), split the dataset S into subsets using the feature(out of the **categorical** remainining features, i.e. unused for splitting up until this node) for which the Information Gain is maximum.\n",
    "    \n",
    "    3. Make a decision tree node using the feature with the maximum Information gain.\n",
    "    \n",
    "    4. If all rows belong to the same class, make the current node as a leaf node with the class as its label.\n",
    "    \n",
    "    5. Repeat for the remaining features until we run out of all features, or the decision tree has all leaf nodes.\n",
    "    \n",
    "    6. In a case where the algorithm runs out of attributes to use and the node is still unable to purify the subset of samples, the node is made a leaf node and labelled with the most common class of the examples in the subset.\n",
    "    \n",
    "    7. depending upon the categories that a feature has, a parent node will have corresponding number of child-nodes\n",
    "    \n",
    "        1. it may so happen that w.r.t. the feature selected for splitting at the current node, there may exist some categories for this feature which have 0 remaining samples out of the subset of remaining training samples.\n",
    "        \n",
    "        2. for such categories, a leaf node is created and labelled with the most common class of the examples in the parent node's set.\n",
    "        \n",
    "## Properties\n",
    "\n",
    "1. ID3 does not guarantee an optimal solution. \n",
    "\n",
    "    1. It can converge upon local optima(<font color=\"red\">and not a global one</font>). \n",
    "    \n",
    "    2. It uses a greedy strategy by selecting the locally best attribute to split the dataset on each iteration.\n",
    "    \n",
    "    3. The algorithm's optimality can be improved by using backtracking during the search for the optimal decision tree at the cost of possibly taking longer.\n",
    "\n",
    "2. ID3 can overfit the training data. \n",
    "    \n",
    "    1. To avoid overfitting, smaller decision trees should be preferred over larger ones.\n",
    "    \n",
    "    2. This algorithm usually produces small trees, but it does not always produce the smallest possible decision tree.\n",
    "\n",
    "3. ID3 is harder to use on continuous data than on categorical data. \n",
    "    \n",
    "    1. If the values of any given attribute are continuous, then there are many more places to split the data on this attribute, and searching for the best value to split by can be time consuming.\n",
    "    \n",
    "4. <font color=\"red\">high memory complexity</font> since <font color=\"red\">no pruning techniques are specified</font> as part of this algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C4.5 \n",
    "\n",
    "1. C4.5 is the successor to ID3 and removed the restriction that features must be categorical by dynamically defining a discrete attribute (based on numerical variables) that partitions the continuous attribute value into a discrete set of intervals. \n",
    "\n",
    "2. C4.5 converts the trained trees (i.e. the output of the ID3 algorithm) into sets of if-then rules.\n",
    "\n",
    "3. This accuracy of each rule is then evaluated to determine the order in which they should be applied.\n",
    "\n",
    "4. Pruning is done by removing a rule’s precondition if the accuracy of the rule improves without it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision-Tree split for numerical variables\n",
    "\n",
    "1. sort all samples w.r.t. a numerical variable\n",
    "\n",
    "2. threshold defined for this numerical variable\n",
    "\n",
    "3. check which values satifsy this threshold, and branch them accordingly\n",
    "    1. for instance, it could be that all samples not satisfying this threshold will be split into the left branch w.r.t. this node, and others that do satisfy this threshold will split into the right branch\n",
    "    \n",
    "    2. hence we would have some samples, in either of the branches, with labels = 1(YES) and labels = 0(NO)\n",
    "\n",
    "4. this splitting is done again, until leaves are reached.\n",
    "\n",
    "5. the estimated time complexity is O(n), hence, if number of records are of the order $10^8$ or higher\n",
    "    1. this is observed in ensemble techniques as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_cpu] *",
   "language": "python",
   "name": "conda-env-tensorflow_cpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
