{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperplane\n",
    "1. 2 planes(only for binary classification) exist on the either side of this hyperplane\n",
    "2. each plane passes through the point belonging in that class and is also nearest to the hyperplane\n",
    "3. both of these planes are parallel to the hyperplane\n",
    "4. <img src=\"svm-1.png\" float=\"left\"/>\n",
    "5. these **points**, as seen above, are called support vectors for each class\n",
    "    1. as seen above, there can exist *multiple support vectors for the same class*\n",
    "6. **Technical Note**: hyerplane will have an `n-1` dimensionality(n: feature dimensionality). observe hyperplane in this 2d feature representation is a line, in a 3d feature representation will be a 2d plane.\n",
    "\n",
    "# Linearly separable\n",
    "it is advisable that SVM be used with linearly separable\n",
    "1. in the above image, the points are arranged in a linearly separable manner\n",
    "2. linearly separable, as it sounds, means that there exists atleast 1 line(or hyperplane, in case of >2D feature-space) that separates a given collection of points, such that points lying at one side belong to one class, and to the other belong to the other one\n",
    "3. <img src=\"linearly_vs_non-linearly_separable.png\" float=\"left\"/> \n",
    "\n",
    "# Fundamental Flow of SVMs\n",
    "## Introduction\n",
    "<font size=5>The primary goal of SVMs is to find the hyperplane that not only separates the classes but also maximizes the margin between them. This is referred to as the maximum margin principle.</font>\n",
    "\n",
    "1. Fundamentally assumes the existence of a **decision boundary** hyperplane that can clearly separate two clusters of points. These points come to be known as *linearly separable* points, because a linear plane is separating them.\n",
    "2. The points on the *either* side of this hyperplane that are closest to it are known as support vectors. In the above example, the red filled squares and the blue filled circle are examples of support vectors on either side of the hyperplane (which is a line in this case).\n",
    "3. A hyperplane parallel to this original hyperplane and passing through the support vectors on the either side is known as a **margin boundary**. For the above binary classification, 2 margin boundaries will exist, beyond which samples will be labelled as belonging to that particular class.\n",
    "4. Hence, we arrive at 3 hyperplanes for a binary classification problem.\n",
    "5. the equation of the hyperplane: $w^T.x + b = 0$\n",
    "    1. 2D feature-space intuition: $x = [x_1 x_2] \\,\\, , \\,\\, w = [w_1 w_2]$\n",
    "        1. a hyperplane in 2D feature space is a ($2-1 = 1$)d hyperplane, i.e. a line.\n",
    "        2. equation of a line is of the form $ax + by + c = 0$. Observe  $w^Tx + b = w_1x_1 + w_2x_2 + b = 0$ is the exact same.\n",
    "    2. this 2D intuition can be extended to higher dimensions as well.\n",
    "6. A particular choice of the normal vector, $\\vec{w}$, decides which samples will be used as the support vectors from either classes.\n",
    "7. After assuming a particular value $\\vec{w}$,  since the support vectors (and all other samples, i.e. points) are fixed points in space, their relative orientation to one another remains the same which makes the distance between the margin boundaries will be a constant. Lets call it K.\n",
    "8. Lets call the distance between the positive margin boundary and the decision boundary as $d_{+}$ and that between the negative margin boundary and decision boundary as $d_{-}$\n",
    "9. Problem-1: infinitely many hyperplanes exist in between the margin boundaries that can partition the space into classes\n",
    "    1. To choose the optimal one, we find that hyperplane that's the farthest from both kinds of support vectors (+ and -)\n",
    "    2. If we assume skewness is the optimal solution, i.e. \n",
    "        1. $d_+ > d_-$, then the decision boundary is closer to the negative class, and the points in the test set that appear in between the margins will be labelled more oftenly as belonging to the positive class. \n",
    "        2. **Remember** that in an **SVM**, the only **real thing** is the **decision boundary**. **On deciding** the decision boundary, i.e. $\\mathbf{f(x) = w^Tx+b}$ , points are classified based on the sign of $f(x)$, if for a given point(sample) $f(x) > 0 \\Rightarrow$ positive class, , if $f(x) < 0 \\Rightarrow$ negative class.\n",
    "        3. lets understand this with an example\n",
    "            1. margins: $ y=x \\,\\,,\\,\\, y = x-5$, decision boundary = $f(x) = x-y-d_+$\n",
    "            2. consider points (3,2), (5,0), (4,1). observe all these points lie in between the margins.\n",
    "                1. for case 1: $d_+=d_-=2.5$,\n",
    "                    1.  point (3,2) will have $f(x) = x-y-d_+ = 3-2-2.5 = -1.5$ which is **less than 0** , hence will be classified as -1. \n",
    "                    2. point (4,1) will have $f(x) = 4-1-2.5 = 0.5$ which is **greater than 0**, hence will be classified as +1. \n",
    "                    3. point (5,0) will have $f(x) = 5-0-2.5 = 2.5$ , which is **greater than 0**, hence will be classified as +1.\n",
    "                2. for case 2: $d_+=4 \\,\\, , \\,\\, d_-=1$, thus decision boundary = $f(x) = x-y-4$\n",
    "                    1. point (3,2) will have $f(x) = x-y-d_+ = 3-2-4 = -3$ which is **less than 0** , hence will be classified as -1. \n",
    "                    2. point (4,1) will have $f(x) = 4-1-4 = -1$ which is **less than 0**, hence will be classified as -1. \n",
    "                    3. point (5,0) will have $f(x) = 5-0-4 = 1$ , which is **greater than 0**, hence will be classified as +1. \n",
    "                    4. In this scenario, the **2nd point flipped because of the skewness towards the negative-class margin** $x_1-x_2-5=0$ (i.e. y=x-5)\n",
    "                3. for case 3: $d_+=1, d_-=4$, s.t. $f(x) = x-y-1$\n",
    "                    1. point (3,2) will have $f(x) = x-y-d_+ = 3-2-1 = 0$ which is **borderline** , hence will not be classified. \n",
    "                    2. point (4,1) will have $f(x) = 4-1-1 = 2$ which is **greater than 0**, hence will be classified as +1. \n",
    "                    3. point (5,0) will have $f(x) = 5-0-1 = 3$ , which is **greater than 0**, hence will be classified as +1. \n",
    "                    4. In this scenario, all the points flipped because of the skewness towards the positive-class margin x1-x2=0 (i.e. y=x)\n",
    "            3. So skewness could in some cases yield a better accuracy score, but in some cases would be worse. \n",
    "    3. **SVM** stays **robust** by \"staying in the between\" i.e. **avoiding skewness**.\n",
    "        1. **Avoiding skewness** corresponds to the $\\mathbf{d_+=d_-=d}$ ,meaning that the **decision boundary** is **equidistant from** the **margins** (support vectors).\n",
    "        2. **Robustness** also implies low variance of predictions, and a reduced risk of overfitting to certain samples or even a new dataset.\n",
    "10. Now, we decide to scale down our coordinates system s.t. $d_+ = d_- = 2d = 2 (\\mathrm{meaning} d = 1)$ because this makes the following calculations quite easier.\n",
    "    1. **Note:** scaling down won't change the relative orientation of the points, just the value of the distance will become smaller, but that's what normalisation is supposed to do.\n",
    "    2. this makes the equation of the margin boundaries as $w^Tx + b = 1 \\,\\, , \\,\\, w^Tx + b = -1$\n",
    "    3. remember support vectors lie on their margin boundaries, assume points $x_+$ and  $x_-$ as the 2 support vectors, then $w^Tx_+ + b = 1 \\,\\, , \\,\\, w^Tx_- + b = -1$\n",
    "    4. the displacement vector from $x_+$ to $x_-$ is $x_+ - x_-$. its component on the normal of the hyperplane (unit vector $\\hat{w}$) will be the same as the distance between the margin boundaries themselves = $(x_+ - x_-).\\hat{w} = \\dfrac{(x_+ - x_-).w}{||w||} = \\dfrac{w.x_+ - w.x_-}{||w||} = \\dfrac{w^Tx_+ - w^Tx_-}{||w||} = \\dfrac{(1-b) - (-1-b)}{||w||} = \\dfrac{2}{||w||}$\n",
    "    5. <font color=\"green\">Side Note:</font> Normalisation was done so that we have a perfect constant (i.e. 2) in the numerator, otherwise it would've been $\\dfrac{2d}{||w||}$\n",
    "11. Hence, distance between margin boundaries = $\\dfrac{2}{||w||}$\n",
    "12. **Keep in mind, we haven't yet concretely defined the decision boundary**.\n",
    "    1. This is because we haven't defined yet defined the optimal normal vector.\n",
    "    2. Hence, even now, there are infinitely many possible solutions.\n",
    "    3. What we have done is to ensure that for SVM to stay robust, maximum margin principle should be obeyed.\n",
    "13. Notice $y_+ = 1  \\,\\,,\\,\\, y_- = -1 \\Rightarrow$, for support vectors $w^Tx_+ + b = 1 \\,\\,,\\,\\, w^Tx_- + b = -1 \\Rightarrow y.(w^Tx+b) = 1$ , and for other points $y.(w^x+b) \\ge 1$\n",
    "14. this makes the problem of SVM as $\\begin{align} & \\text{maximize} \\quad \\dfrac{2}{||w||} \\\\ & \\text{subject to} \\\\ & y_i.(w^Tx_i+b) \\ge 1 \\,\\, \\forall \\,\\, i \\, \\epsilon \\, 1,2, \\cdots , n_{samples} \\end{align}$ , which is the same as $\\begin{align} & \\text{minimize} \\quad \\dfrac{||w||}{2} \\\\ & \\text{subject to} \\\\ & y_i.(w^Tx_i+b) \\ge 1 \\,\\, \\forall \\,\\, i \\, \\epsilon \\, 1,2, \\cdots , n_{samples} \\end{align}$\n",
    "15. Hence, **the magnitude of the normal vector that defines the hyperplane should be minimised as per an SVM model**.\n",
    "16. Notice that the current SVM algorithm allows 0 room of error, hence is called **hard margin** SVM.\n",
    "    <img src=\"./images/svm_with_outliers.png\" width=200/>\n",
    "    1. a **soft margin** SVM on the other hand allows room for small errors. <img src=\"./images/svm_soft_margin.png\" width=200/>\n",
    "    2. the **soft margin SVM** can be written as $\\begin{align} & \\text{minimize} \\quad \\dfrac{||w||}{2} + \\sum\\limits_{i=1}^{n_{samples}} \\xi_i \\\\ & \\text{subject to} \\\\ & y_i.(w^Tx_i+b) \\ge 1 \\,\\, \\forall \\,\\, i \\, \\epsilon \\, 1,2, \\cdots , n_{samples} \\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lagrangian Duality: Primal to Lagrangian Conversion\n",
    "1. we saw the **Primal** form of the optimisation function for binary classification in SVMs\n",
    "    1. its called primal because its subject to constraints.\n",
    "    2. also, minimize $\\frac{||w||}{2}$ == $\\frac{||w||^2}{2}$, so we use the latter to avoid any square roots.\n",
    "2. if strong duality holds true, then the primal problem could be converted to a dual problem and be solved\n",
    "    1. dual problem is preferred as it incorporates all equality and inequality constraints associated with the primal problem into a single function, the Lagrangian.\n",
    "    2. refer this [maths notebook](../notes/Maths.ipynb)\n",
    "3. $\\begin{align} \\mathcal{L}(w, b, \\alpha) = \\dfrac{||w||^2}{2} - \\sum\\limits_{i=1}^{n_{samples}} \\alpha_i.\\left[y_i.\\left(w^Tx_i + b\\right) - 1 \\right] \\end{align}$\n",
    "    - a difference instead of a sum because conventionally the inequality constraints are assumed to be non-positive, and that would correspond to $1-y_i(w^Tx_i+b) \\le 0$\n",
    "        - even if you keep it as is, thus making the lagrangian an addition of two terms, the stationarity makes $w = -\\sum\\limits_{i=1}^{n_{samples}} \\alpha_iy_ix_i$ , which when substituted back in the addition form of lagrangian (the second term) makes the second term identical to what we have here.\n",
    "    - **keep in mind** that this is a **linear programming problem** because the **constraints are linear** (power of $w, b, \\alpha$ is 1 in all the constraints)\n",
    "    - if a solution exists in the dual state, it should \n",
    "        - obey the original constraints (primal feasibility), \n",
    "        - it should obey stationarity (which is how we will arrive at the optimal solution for $w$), \n",
    "        - it should obey duality (all $\\mathbf{\\alpha_i} \\ge 0$) and \n",
    "        - complementary slackness (if a sample is on the boundary, i.e. a support vector, then the constraint is active and $\\alpha_i > 0$ strictly, else if a sample is in the feasible region then $\\alpha_i = 0$ because the point is correctly classified and shouldn't be penalised)\n",
    "4. Stationarity: $\\begin{align} \\dfrac{\\partial \\mathcal{L}}{\\partial w} = w - \\sum\\limits_{i=1}^{n_{samples}} \\alpha_i.y_i.x_i = 0 \\\\  \\Rightarrow w = \\sum\\limits_{i=1}^{n_{samples}} \\alpha_i.y_i.x_i \\\\ \\Rightarrow ||w||^2 = w^Tw = \\sum\\limits_{i=1}^{n_{samples}} \\alpha_i.y_i.x_i^T.\\sum\\limits_{j=1}^{n_{samples}} \\alpha_j.y_j.x_j \\\\ = \\sum\\limits_{i=1}^{n_{samples}}\\sum\\limits_{j=1}^{n_{samples}} \\left(\\alpha_i.y_i.x_i^T.\\alpha_j.y_j.x_j \\right) \\\\ =   \\sum\\limits_{i=1}^{n_{samples}}\\sum\\limits_{j=1}^{n_{samples}} \\left(\\alpha_i\\alpha_jy_iy_jx_i^Tx_j \\right) \\\\ \\text{ remember first term = } \\dfrac{||w||^2}{2} = \\frac{1}{2}\\left(\\sum\\limits_{i=1}^{n_{samples}}\\sum\\limits_{j=1}^{n_{samples}} \\left(\\alpha_i\\alpha_jy_iy_jx_i^Tx_j \\right) \\right) \\\\  \\dfrac{\\partial \\mathcal{L}}{\\partial b} = \\sum\\limits_{i=1}^{n_{samples}} \\alpha_i.y_i = 0 \\\\ \\text{ this makes } \\mathcal{L}(w, \\alpha) = \\text{first term} - \\sum\\limits_{i=1}^{n_{samples}} \\alpha_i.\\left[y_i.\\left(\\sum\\limits_{j=1}^{n_{samples}} \\alpha_j.y_j.x_j^T x_i + b\\right) - 1 \\right]  \\\\  = \\text{ first term } - \\left(\\sum\\limits_{i=1}^{n_{samples}}\\sum\\limits_{j=1}^{n_{samples}} \\left(\\alpha_i\\alpha_jy_iy_jx_i^Tx_j \\right) + \\sum\\limits_{i=1}^{n_{samples}} \\alpha_iy_ib - \\sum\\limits_{i=1}^{n_{samples}} \\alpha_i \\right) \\\\  = \\text{ first term } - 2\\times\\text{ first term } - b\\sum\\limits_{i=1}^{n_{samples}} \\alpha_iy_i + \\sum\\limits_{i=1}^{n_{samples}} \\alpha_i \\\\ = \\sum\\limits_{i=1}^{n_{samples}} \\alpha_i - \\text{ first term } \\\\ \\dfrac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = y_i(w^Tx_i+b)-1=0  \\\\ \\Rightarrow y_i(w^Tx_i+b) = 1 \\\\ \\text{ Substituting these relations in the main Lagrangian } \\\\ \\mathcal{L}(w, b, \\alpha) = \\sum\\limits_{i=1}^{n_{samples}} \\alpha_i - \\frac{1}{2}\\left(\\sum\\limits_{i=1}^{n_{samples}}\\sum\\limits_{j=1}^{n_{samples}} \\left(\\alpha_i\\alpha_jy_iy_jx_i^Tx_j \\right) \\right)   \\end{align}$\n",
    "5. Complementary slackness:\n",
    "    - states that when constraints are inactive, i.e. w.r.t. points **strictly** in the feasible region ($y_i(w^Tx_i + b) > 1$), $\\alpha_i = 0$ i.e. relax the penalisation\n",
    "    - and for boundary points, i.e. $y_i(w^Tx_i + b) = 1$, constraints become active hence penalise these, which means $\\alpha_i > 0$. These points are nothing but the support vectors of each class.\n",
    "6. So from stationarity and complementary slackness, we get the problem of **maximising the lagrangian on only the support vectors**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "- the sign of the function $f(x) = w^Tx + b$ tells us which class a given sample belongs to.\n",
    "- its because *positive* samples meant that their $f(x) > 0$, i.e. lied on the positive region of the decision boundary and *negative* samples on the other side, i.e. the negative side.\n",
    "- so, for a given sample, if $w^Tx + b > 0 \\Rightarrow + \\,\\,,\\,\\, w^Tx + b < 0 \\Rightarrow - $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Score\n",
    "- [depends whether](https://github.com/scikit-learn/scikit-learn/blob/99bf3d8e4eed5ba5db19a1869482a238b6223ffd/sklearn/svm/_base.py#L539) the sample space is [sparse](https://github.com/scikit-learn/scikit-learn/blob/99bf3d8e4eed5ba5db19a1869482a238b6223ffd/sklearn/svm/_base.py#L189) or not\n",
    "\n",
    "### Sparse\n",
    "\n",
    "### Dense\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM-flow in Multi-class classification\n",
    "- usually 2 methods are used: One vs One and One vs Rest\n",
    "\n",
    "### One-vs-One\n",
    "- each pair of classes are separated by a decision boundary uniquely determined using their respective samples\n",
    "- $^nC_2$ total decision boundaries, each being a binary SVM classifier\n",
    "- classifier $S_{ij}: f(x) = w_{ij}^Tx+b_{ij}$ defined for a given pair of classes $C_i, C_j$.\n",
    "    - $f(x) > 0 \\Rightarrow C_i \\textrm{ else } C_j$\n",
    "- $^nC_2$ classes will be inferred for a given point at inference time\n",
    "    - the final class is decided via voting.\n",
    "    - if say there are multiple classes with the highest no. of votes, cumulative confidence score shall be used\n",
    "    - confidence score is \n",
    "\n",
    "## SVM-flow in regression\n",
    "<font color=\"red\">Regression: how would the hyperplanes-based zoning correspond to a value being predicted?</font>\n",
    "\n",
    "\n",
    "# Incorporating deviation for misclassified samples\n",
    "1. it may so happen that $\\textrm{y}^{(\\textrm{i})}$ = 1, but $w^{\\textrm{T}}$x$^{(\\textrm{i})} + b$ is not $\\ge$ 1 , i.e. our SVM model has missclassified the sample.\n",
    "2. hence, we need to define error $\\xi^{(\\textrm{i})}$ = 1 - $\\textrm{y}^{(\\textrm{i})}$.($w^{\\textrm{T}}$$x^{(\\textrm{i})} + b$) for each sample i.\n",
    "    1. its assumed that $\\xi^{(\\textrm{i})} \\ge 0$, hence error-term for points correctly classified is not taken into consideration.\n",
    "3. this captures the errors from the assumption that **data is linearly separable**\n",
    "4. hence, now the optimisation function becomes : min.($\\frac{||w||^2}{2}$ + $C \\sum\\limits_{i=1}^{n}\\xi^{(\\textrm{i})}$), for a total of n training samples, subject to the constraints: \n",
    "$\\xi^{(\\textrm{i})} \\ge 0$ and $\\xi^{(\\textrm{i})}$ = 1 - $\\textrm{y}^{(\\textrm{i})}$.($w^{\\textrm{T}}x^{(\\textrm{i})} + b$).\n",
    "\n",
    "## Lagrangian method of multipliers\n",
    "1. The idea used in Lagrange multiplier is that the gradient of the objective function f, lines up either in parallel or anti-parallel direction to the gradient of the constraint g, at an optimal point. \n",
    "2. In such case, one the gradients should be some multiple of another.\n",
    "3. \n",
    "\n",
    "# Importance of support vectors\n",
    "1. Deleting the support vectors will change the position of the hyperplane. \n",
    "2. These are the points that help us build our SVM.\n",
    "\n",
    "\n",
    "# sklearn.svm.SVC\n",
    "1. The fit time scales at least quadratically with the number of samples and may be <font color=\"red\">impractical beyond tens of thousands of samples</font>. \n",
    "2. For **large datasets** consider using [sklearn.svm.LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) or [sklearn.linear_model.SGDClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier) instead, possibly after a [sklearn.kernel_approximation.Nystroem](https://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.Nystroem.html#sklearn.kernel_approximation.Nystroem) transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM kernels\n",
    "1. used to convert low dimensional feature-space to a higher one\n",
    "2. primarily used to convert non-linearly separable data to linearly-separable one"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_cpu] *",
   "language": "python",
   "name": "conda-env-tensorflow_cpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
