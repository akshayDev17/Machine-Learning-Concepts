{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# XGBoost Regressor\n",
    "1. loss function = mean square error, i.e. $\\mathcal{L} = \\sum\\limits_{i=1}^N \\dfrac{(y_i - \\hat{y}_i)^2}{N}$\n",
    "2. Negative gradient = $-\\nabla \\mathcal{L}$ (w.r.t. $\\hat{y}_i$) $= \\sum\\limits_{i=1}^N \\dfrac{2(y_i-\\hat{y}_i)}{N}$\n",
    "3. Hessian (2nd order derivative) = $H(\\mathcal{L}) = 2$\n",
    "4. model-0 = mean-value predictor\n",
    "5. calculate residuals $r_i = y_i - \\bar{y}$\n",
    "6. for the 1st weak learner decision tree, splitting is as follows\n",
    "    1. **no MSE reduction will be used**\n",
    "    2. calculate similarity score at current node $SS = \\frac{G_N^2}{H_N+ \\lambda}$, where $\\lambda$ : regularisation parameter \n",
    "        1. $SS = \\sum\\limits_{i=1}^N \\dfrac{\\dfrac{2(y_i-\\hat{y}_i)^2}{N}}{2 + \\lambda} = \\sum\\limits_{i=1}^N \\dfrac{(y_i-\\hat{y}_i)^2}{N+ N\\lambda/2} = \\dfrac{\\sum\\limits_{i=1}^N r_i^2}{N+ N\\lambda/2}$ \\\n",
    "            = (sum of residuals from previous learner)/(total no. of residuals, i.e. samples in the current node)\n",
    "    3. use a split criterion for a given feature to obtain right and left child nodes, and thus samples that end up in them, use those respective samples to calculate $SS_R \\,,\\, SS_L$\n",
    "    4. find the split gain as $SS_R + SS_L - SS$, and the splitting criterion with the highest gain will be used\n",
    "7. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Classifier"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
