{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# XGBoost Regressor\n",
    "1. loss function = mean square error, i.e. $\\mathcal{L} = \\sum\\limits_{i=1}^N \\dfrac{(y_i - \\hat{y}_i)^2}{N}$\n",
    "2. Negative gradient = $-\\nabla \\mathcal{L}$ (w.r.t. $\\hat{y}_i$) $= \\sum\\limits_{i=1}^N \\dfrac{2(y_i-\\hat{y}_i)}{N}$\n",
    "3. Hessian (2nd order derivative) = $H(\\mathcal{L}) = 2$\n",
    "4. model-0 = mean-value predictor\n",
    "5. calculate residuals $r_i = y_i - \\bar{y}$\n",
    "6. for the 1st weak learner decision tree, splitting is as follows\n",
    "    1. **no MSE reduction will be used**\n",
    "    2. calculate similarity score at current node $SS = \\frac{G_N^2}{H_N+ \\lambda}$, where $\\lambda$ : regularisation parameter \n",
    "        1. $SS = \\sum\\limits_{i=1}^N \\dfrac{\\dfrac{2(y_i-\\hat{y}_i)^2}{N}}{2 + \\lambda} = \\sum\\limits_{i=1}^N \\dfrac{(y_i-\\hat{y}_i)^2}{N+ N\\lambda/2} = \\dfrac{\\sum\\limits_{i=1}^N r_i^2}{N+ N\\lambda/2}$ \\\n",
    "            = (sum of residuals from previous learner)/(total no. of residuals, i.e. samples in the current node)\n",
    "    3. use a split criterion for a given feature to obtain right and left child nodes, and thus samples that end up in them, use those respective samples to calculate $SS_R \\,,\\, SS_L$\n",
    "    4. find the split gain as $SS_R + SS_L - SS$, and the splitting criterion with the highest gain will be used\n",
    "7. \n",
    "\n",
    "## Differences from GBM\n",
    "1. L1 and L2 regularisation (the $\\lambda$ term in the similarity score expression)\n",
    "    1. $\\lambda$ if high, will cause all the 3 terms to be quite small, leading to a higher bias but lower variance model\n",
    "    2. hence it is curing overfitting\n",
    "    \n",
    "2. optimised for speed, efficiency, scalability\n",
    "    1. xgboost uses 2nd order taylor series expansion for loss approximation\n",
    "    2. column/block-based parallelisation\n",
    "        Consider a small dataset with one feature (Feature X), a target variable (Y), and corresponding gradient and Hessian values for each sample. \n",
    "\n",
    "        | Sample  | Feature X | Target (Y) | Gradient (g) | Hessian (h) |\n",
    "        |---------|-----------|------------|--------------|-------------|\n",
    "        | 1       | 2.1       | 10         | -0.8         | 1.2         |\n",
    "        | 2       | 2.5       | 12         | -0.6         | 1.1         |\n",
    "        | 3       | 3.2       | 15         | 0.2          | 0.9         |\n",
    "        | 4       | 3.7       | 18         | 0.4          | 1           |\n",
    "        | 5       | 4         | 20         | 0.5          | 1.3         |\n",
    "\n",
    "        1. discretize feature values into histogram bins \n",
    "\n",
    "            | Bin Range  | Feature values mapped |\n",
    "            |---------|-----------|\n",
    "            | 1: (2.0-2.7]   | 2.1, 2.5       |\n",
    "            | 2: (2.7 - 3.5) | 3.2       |\n",
    "            | 3: (3.5 - 4.2] | 3.7, 4    |\n",
    "\n",
    "        3. we sum the gradient (g) and Hessian (h) values \n",
    "\n",
    "            | Bin Range  | Gradient Sum(G) | Hessian sum (H) |\n",
    "            |---------|-----------|--------------|\n",
    "            | 1: (2.0-2.7]   | -0.8 + -0.6 = -1.4   |  1.2 + 1.1 = 2.3 |\n",
    "            | 2: (2.7 - 3.5) | 0.2       |   0.9   |\n",
    "            | 3: (3.5 - 4.2] | 0.4 + 0.5 = 0.9    |   1 + 1.3 = 2.3 |\n",
    "\n",
    "        4. Let's compute the gain for splitting after Bin 1: \n",
    "            - left ( i.e. bin 1 ): $G_L = -1.4 \\,,\\, H_L = 2.3$\n",
    "            - right ( i.e. bin 2 and bin 3 ): $G_R = 0.2  + 0.9 = 1.1 \\,,\\, H_R = 0.9 + 2.3 = 3.2$\n",
    "            - $Gain = \\frac{G_L^2}{H_L+ \\lambda} + \\frac{G_R^2}{H_R+ \\lambda} - \\frac{G_N^2}{H_N+ \\lambda} = \\frac{(-1.4)^2}{2.3+ \\lambda} + \\frac{(1.1)^2}{3.2+ \\lambda} - \\frac{(-1.4 + 1.1)^2}{2.3 + 3.2 + \\lambda}$\n",
    "        5. Similarly, gain is computed for other split points (after Bin 2 [left = bin1 + bin2, right = bin 3]), and the split with the highest gain is selected.\n",
    "        6. Benefits of Histogram-Based Splitting\n",
    "            1. Speed Improvement:\n",
    "                1. Instead of checking all possible splits, we only evaluate a limited number of bins.\n",
    "                2. Reduces the number of comparisons from O(n) (raw splits) to O(#bins).\n",
    "            2. Memory Efficiency:\n",
    "                1. Storing bin statistics is significantly smaller than storing raw feature values.\n",
    "            3. Robustness to Noisy Data:\n",
    "                1. Since histogram bins aggregate values, small fluctuations in feature values have minimal impact.\n",
    "            4. **Handling missing values** : While constructing histograms for each feature, XGBoost only considers the non-missing values for that feature, and skips missing values entirely, which helps in two ways:\n",
    "                1. Reduces Computation: For sparse features, this speeds up histogram updates because it doesnâ€™t need to evaluate missing or zero entries.\n",
    "                2. Memory Efficiency: By not allocating space or performing operations for missing values, it reduces the amount of memory needed.\n",
    "        7. When evaluating potential splits for a feature, XGBoost **assigns missing values** to **either** the **left or right child** of a split depending on how it maximizes the gain.\n",
    "            - Best split strategy for missing values: XGBoost tries both assignments (left and right) and chooses the one that results in the best gain.\n",
    "            - This behavior allows it to handle missing values naturally without the need for imputation or extra preprocessing, unlike many other algorithms.\n",
    "            - For example, if a feature has 100 samples and only 20 are non-zero or non-missing, XGBoost will only compute the statistics for these 20 samples and ignore the 80 missing/zero entries during the tree construction phase.\\\n",
    "                This reduces the number of operations and speeds up computation because it avoids wasting resources on irrelevant entries.\n",
    "3. XGBoost can learn how to handle missing values automatically (it determines the best direction to split for missing data).\n",
    "4. Preferred for larger datasets since computationally optimized\n",
    "5. Preferred when GBM tends to overfit\n",
    "6. Faster inference time than GBM\n",
    "7. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Classifier"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
