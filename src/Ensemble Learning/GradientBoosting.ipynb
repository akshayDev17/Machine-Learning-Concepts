{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Regression\n",
    "1. chain of models, where the successive models are trained on the *mistakes* of the preceding ones\n",
    "2. begin with a mean-value model: mean of target value across all training samples = estimated value\n",
    "    1. this mean value model will be called $\\mathbf{F_0(x)}$\n",
    "    2. calculate pseudoresiduals:\n",
    "        1. for each sample in the training set, find PR = $y_i - \\hat{y}_i$, and since $\\hat{y}_i = \\bar{y}$, \n",
    "            1. notice that the regression loss function, MSE, = $\\sum\\limits_{i=1}^N\\dfrac{(y_i - \\hat{y}_i)^2}{N}$, and $\\nabla \\mathcal{L} = -2\\sum\\limits_{i=1}^N\\dfrac{(y_i - \\hat{y}_i)}{N} = 2\\sum\\limits_{i=1}^N\\dfrac{(\\hat{y}_i - y_i)}{N}$\n",
    "            2. notice the relation between the gradient of the loss function for a given sample, and the pseuoresidual of that sample.\n",
    "            3. this is why, pseudoresidual is the negative gradient of MSE (regression loss function)\n",
    "    3. use the pseudoresiduals as the **labels** that would be predicted by the successor model\n",
    "3. usually DecisionTrees will be used as the weak learners after this mean-value model\n",
    "4. a weak learner, i.e. a shallow decision tree , with small no. of nodes, is used:\n",
    "    1. this is the first weak learner, i.e. $\\mathbf{h_1(x)}$ and will now be fit on the original training data but using the PR's from the preceding model as the target.\n",
    "    2. the predictions from this learner ($\\mathbf{h_1(x)}$) will now be calculated.\n",
    "    3. the prediction of the actual target variable at this stage, as given by the gradient boosting model built until now, will be $\\mathbf{F_1(x)} = \\mathbf{F_0(x)} + \\eta\\times\\mathbf{h_1(x)}$, where $\\eta$: learning rate hyperparameter.\n",
    "    4. PR($F_1(x_i)$) = $y_i - F_1(x_i)$\n",
    "    5. this will now serve as the target to train model 2, i.e. $h_2(x)$, upon which $\\mathbf{F_2(x)} = \\mathbf{F_0(x)} + \\eta\\times\\mathbf{h_1(x)} + \\eta\\times\\mathbf{h_2(x)} = F_1(x) + + \\eta\\times\\mathbf{h_2(x)}$\n",
    "    6. this is continued\n",
    "    7. **Note:** that the samples in the nodes of these weak learners will have the pseudoresiduals from the previous learners within them, as opposed to the target variable y values that are generally stored in the nodes of a tree based algorithm/ensemble.\n",
    "5. so after adding M models, $\\mathcal{F}(x) = F_0(x) + \\sum\\limits_{m=1}^M \\eta.h_m(x)$\n",
    "6. **no bootstrapping, aggregation == boosting:**\n",
    "7. **gradient** because the model gradually moves in the direction of the steepest descent w.r.t. the loss function, **boosting** due to the sequential combination of predictions from weak learners.\n",
    "\n",
    "## Mathematical intuition\n",
    "- observe that for a perfect model with only 1 weak learner\n",
    "    - the mean-value model will make some errors, which will be accurately predicted by the 1st weak learner\n",
    "    - if learning rate = 1, $F_1(x) = y$ (think)\n",
    "- if say the first weak learner wasn't perfect, but the 2nd learner was:\n",
    "    - $F_2(x) = y$ (think)\n",
    "- the basic idea behind the successive weak learner being trained using the residual of the previous weak learner is to minimize the eventual residual\n",
    "    - observe that each new learner will theoretically take the residual as closer to 0 as it can.\n",
    "\n",
    "## Why all underlying estimators are weak?\n",
    "- to control overfitting, they are essentially *weak*\n",
    "- either tree depth is constrained, or no. of leaf nodes. (Basically any sort of hyperparametric pre-pruning)\n",
    "- each weak learner: computationally less expensive to train\n",
    "- diversity: since the target is changing, different patterns are being learnt by each added weak learner.\n",
    "- *weakness* is a form of regularizing DTs\n",
    "- `subsample` in [`sklearn.GradientBoostingRegressor`](https://scikit-learn.org/dev/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html) can be used to only use a random subset of samples to train the weak learners. <font color=\"red\">Check if for each new weak learner does the sampling take place, or is it done once and then this subset is used for all weak learners!!</font>\n",
    "- <font color=\"red\">Why `freidman_mse` over `mse`?</font>\n",
    "\n",
    "## Why learning rate?\n",
    "- low learning rate: more no. of models added <font color=\"red\">Test this!</font>\n",
    "- smoothens the learning process\n",
    "- just another method of reducing the risk of overfitting\n",
    "\n",
    "## Convergence\n",
    "- Hyperparameter based\n",
    "    - `n_estimators`: once these many estimators are built, job done. allows till `inf`.\n",
    "    - `warm_start = True`\n",
    "    - `n_iter_no_change`: performs early stopping if validation score doesn't improve till for these many iterations. `validation_fraction` by default is `0.1`, so we already have a validation set within the model.\n",
    "    - `tol`: tolerance for early stopping, the minimum improvement in validation score to constitute a change "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top correlated features with GBR residuals:\n",
      " Residual_GBR    1.000000\n",
      "Residual_RFR    0.853233\n",
      "Feature_7       0.406423\n",
      "Feature_8       0.387319\n",
      "Feature_9       0.315151\n",
      "Feature_1       0.282697\n",
      "Feature_0       0.040919\n",
      "Feature_6       0.026834\n",
      "Feature_4       0.001473\n",
      "Feature_3       0.000838\n",
      "Feature_2      -0.031012\n",
      "Feature_5      -0.049147\n",
      "Name: Residual_GBR, dtype: float64\n",
      "Top correlated features with RFR residuals:\n",
      " Residual_RFR    1.000000\n",
      "Residual_GBR    0.853233\n",
      "Feature_9       0.510706\n",
      "Feature_8       0.485163\n",
      "Feature_1       0.451374\n",
      "Feature_7       0.298766\n",
      "Feature_6       0.027480\n",
      "Feature_0       0.023553\n",
      "Feature_3       0.012446\n",
      "Feature_4       0.010340\n",
      "Feature_2      -0.007428\n",
      "Feature_5      -0.068697\n",
      "Name: Residual_RFR, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X, y = make_regression(n_samples=1000, n_features=10, n_informative=4, noise=0.2, random_state=42)\n",
    "\n",
    "# Simulate some data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train models\n",
    "gbr = GradientBoostingRegressor().fit(X_train, y_train)\n",
    "rfr = RandomForestRegressor().fit(X_train, y_train)\n",
    "\n",
    "# Residuals\n",
    "residuals_gbr = y_test - gbr.predict(X_test)\n",
    "residuals_rfr = y_test - rfr.predict(X_test)\n",
    "\n",
    "# Correlation with residuals\n",
    "residuals_df = pd.DataFrame({\n",
    "    'Residual_GBR': residuals_gbr,\n",
    "    'Residual_RFR': residuals_rfr,\n",
    "    **{f'Feature_{i}': X_test[:, i] for i in range(X_test.shape[1])}\n",
    "})\n",
    "\n",
    "correlation_gbr = residuals_df.corr()['Residual_GBR'].sort_values(ascending=False)\n",
    "correlation_rfr = residuals_df.corr()['Residual_RFR'].sort_values(ascending=False)\n",
    "\n",
    "print(\"Top correlated features with GBR residuals:\\n\", correlation_gbr)\n",
    "print(\"Top correlated features with RFR residuals:\\n\", correlation_rfr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Residual_GBR</th>\n",
       "      <th>Residual_RFR</th>\n",
       "      <th>Feature_0</th>\n",
       "      <th>Feature_1</th>\n",
       "      <th>Feature_2</th>\n",
       "      <th>Feature_3</th>\n",
       "      <th>Feature_4</th>\n",
       "      <th>Feature_5</th>\n",
       "      <th>Feature_6</th>\n",
       "      <th>Feature_7</th>\n",
       "      <th>Feature_8</th>\n",
       "      <th>Feature_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.811513</td>\n",
       "      <td>17.350903</td>\n",
       "      <td>-1.345014</td>\n",
       "      <td>-0.356092</td>\n",
       "      <td>0.109572</td>\n",
       "      <td>-0.861050</td>\n",
       "      <td>0.001977</td>\n",
       "      <td>1.532873</td>\n",
       "      <td>-0.416438</td>\n",
       "      <td>0.954085</td>\n",
       "      <td>1.218480</td>\n",
       "      <td>2.202014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.008523</td>\n",
       "      <td>10.085189</td>\n",
       "      <td>0.613518</td>\n",
       "      <td>2.142270</td>\n",
       "      <td>1.727543</td>\n",
       "      <td>-1.022793</td>\n",
       "      <td>0.038003</td>\n",
       "      <td>0.120031</td>\n",
       "      <td>0.436324</td>\n",
       "      <td>0.522835</td>\n",
       "      <td>-0.573700</td>\n",
       "      <td>-0.024355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-4.172355</td>\n",
       "      <td>-6.593240</td>\n",
       "      <td>-0.016423</td>\n",
       "      <td>-0.730930</td>\n",
       "      <td>-0.033127</td>\n",
       "      <td>1.188393</td>\n",
       "      <td>-0.517611</td>\n",
       "      <td>0.223788</td>\n",
       "      <td>1.794558</td>\n",
       "      <td>-0.176947</td>\n",
       "      <td>-0.798297</td>\n",
       "      <td>-1.379319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25.017706</td>\n",
       "      <td>30.552690</td>\n",
       "      <td>0.359739</td>\n",
       "      <td>2.824331</td>\n",
       "      <td>-1.318968</td>\n",
       "      <td>-0.498634</td>\n",
       "      <td>0.179622</td>\n",
       "      <td>1.345148</td>\n",
       "      <td>0.020106</td>\n",
       "      <td>2.497415</td>\n",
       "      <td>0.046006</td>\n",
       "      <td>-0.641114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.252838</td>\n",
       "      <td>9.395049</td>\n",
       "      <td>-0.454548</td>\n",
       "      <td>1.023531</td>\n",
       "      <td>0.175287</td>\n",
       "      <td>-0.218653</td>\n",
       "      <td>-0.411823</td>\n",
       "      <td>0.131928</td>\n",
       "      <td>-1.336725</td>\n",
       "      <td>1.695723</td>\n",
       "      <td>1.897289</td>\n",
       "      <td>0.156694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>-1.768974</td>\n",
       "      <td>-0.001022</td>\n",
       "      <td>-1.715180</td>\n",
       "      <td>0.282036</td>\n",
       "      <td>-0.152855</td>\n",
       "      <td>0.825924</td>\n",
       "      <td>1.762980</td>\n",
       "      <td>-0.507717</td>\n",
       "      <td>0.433562</td>\n",
       "      <td>-0.879644</td>\n",
       "      <td>-0.820328</td>\n",
       "      <td>-0.223026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>3.624788</td>\n",
       "      <td>5.315424</td>\n",
       "      <td>-0.527994</td>\n",
       "      <td>0.099314</td>\n",
       "      <td>-0.390409</td>\n",
       "      <td>1.353069</td>\n",
       "      <td>-0.652528</td>\n",
       "      <td>-0.611177</td>\n",
       "      <td>0.516144</td>\n",
       "      <td>-0.074345</td>\n",
       "      <td>-0.184551</td>\n",
       "      <td>0.592330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>-2.056100</td>\n",
       "      <td>-1.272203</td>\n",
       "      <td>0.683329</td>\n",
       "      <td>-1.237662</td>\n",
       "      <td>0.869156</td>\n",
       "      <td>-0.209314</td>\n",
       "      <td>-0.763286</td>\n",
       "      <td>0.037938</td>\n",
       "      <td>0.887291</td>\n",
       "      <td>0.075955</td>\n",
       "      <td>0.742682</td>\n",
       "      <td>0.482249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>1.985634</td>\n",
       "      <td>4.225181</td>\n",
       "      <td>-1.239562</td>\n",
       "      <td>0.258770</td>\n",
       "      <td>1.195531</td>\n",
       "      <td>-1.502871</td>\n",
       "      <td>0.504582</td>\n",
       "      <td>0.626538</td>\n",
       "      <td>-1.518970</td>\n",
       "      <td>0.135563</td>\n",
       "      <td>0.728519</td>\n",
       "      <td>0.637606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>3.876368</td>\n",
       "      <td>-1.886638</td>\n",
       "      <td>-1.179369</td>\n",
       "      <td>-1.611567</td>\n",
       "      <td>-0.262660</td>\n",
       "      <td>0.172254</td>\n",
       "      <td>0.960305</td>\n",
       "      <td>0.462484</td>\n",
       "      <td>-0.336387</td>\n",
       "      <td>1.855503</td>\n",
       "      <td>0.224235</td>\n",
       "      <td>-0.346427</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Residual_GBR  Residual_RFR  Feature_0  Feature_1  Feature_2  Feature_3  \\\n",
       "0        8.811513     17.350903  -1.345014  -0.356092   0.109572  -0.861050   \n",
       "1       -0.008523     10.085189   0.613518   2.142270   1.727543  -1.022793   \n",
       "2       -4.172355     -6.593240  -0.016423  -0.730930  -0.033127   1.188393   \n",
       "3       25.017706     30.552690   0.359739   2.824331  -1.318968  -0.498634   \n",
       "4       -2.252838      9.395049  -0.454548   1.023531   0.175287  -0.218653   \n",
       "..            ...           ...        ...        ...        ...        ...   \n",
       "195     -1.768974     -0.001022  -1.715180   0.282036  -0.152855   0.825924   \n",
       "196      3.624788      5.315424  -0.527994   0.099314  -0.390409   1.353069   \n",
       "197     -2.056100     -1.272203   0.683329  -1.237662   0.869156  -0.209314   \n",
       "198      1.985634      4.225181  -1.239562   0.258770   1.195531  -1.502871   \n",
       "199      3.876368     -1.886638  -1.179369  -1.611567  -0.262660   0.172254   \n",
       "\n",
       "     Feature_4  Feature_5  Feature_6  Feature_7  Feature_8  Feature_9  \n",
       "0     0.001977   1.532873  -0.416438   0.954085   1.218480   2.202014  \n",
       "1     0.038003   0.120031   0.436324   0.522835  -0.573700  -0.024355  \n",
       "2    -0.517611   0.223788   1.794558  -0.176947  -0.798297  -1.379319  \n",
       "3     0.179622   1.345148   0.020106   2.497415   0.046006  -0.641114  \n",
       "4    -0.411823   0.131928  -1.336725   1.695723   1.897289   0.156694  \n",
       "..         ...        ...        ...        ...        ...        ...  \n",
       "195   1.762980  -0.507717   0.433562  -0.879644  -0.820328  -0.223026  \n",
       "196  -0.652528  -0.611177   0.516144  -0.074345  -0.184551   0.592330  \n",
       "197  -0.763286   0.037938   0.887291   0.075955   0.742682   0.482249  \n",
       "198   0.504582   0.626538  -1.518970   0.135563   0.728519   0.637606  \n",
       "199   0.960305   0.462484  -0.336387   1.855503   0.224235  -0.346427  \n",
       "\n",
       "[200 rows x 12 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "residuals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Residuals vs predictions\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(gbr.predict(X_test), residuals_gbr, alpha=0.5, label='GBR Residuals')\n",
    "plt.axhline(0, color='r', linestyle='--')\n",
    "plt.xlabel('Predictions (GBR)')\n",
    "plt.ylabel('Residuals')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(rfr.predict(X_test), residuals_rfr, alpha=0.5, label='RFR Residuals')\n",
    "plt.axhline(0, color='r', linestyle='--')\n",
    "plt.xlabel('Predictions (RFR)')\n",
    "plt.ylabel('Residuals')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variation of model performance with learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classic Scenarios where GBM fails\n",
    "- limited data, noisy data, too many unhandled outliers\n",
    "- dataset drift, if no retraining pipeline established\n",
    "- imbalanced data (without SMOTE)\n",
    "- **datasets with too many features**: may not fail necessarily, but like tree algorithms take a lot of time to train the model\n",
    "    - the **time could be wasted** in case of **too many redundant features**\n",
    "- \n",
    "\n",
    "# Potentially failing scenarios\n",
    "1. y = x1 XOR x2\n",
    "2. y = sin(x1 + x2) - use maclaurin series (engineer features such as x1^2, x2^2, (x1+x2)^2....)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Classifier - Binary Classification\n",
    "1. current discussion limited to binary classification only.\n",
    "2. labels = 0,1\n",
    "3. base model $F_0(x) = log_e\\left(\\frac{p}{1-p}\\right)$, known as log-odds. odds of positive class against the negative class\n",
    "    1. $p$ is simply the ratio of no. of positive samples to the total no. of samples\n",
    "4. hence, for base model, $p = \\dfrac{1}{1+e^{-log\\, odds}}$\n",
    "5. pseudoresiduals, PR = $y_i - p$, which are technically numbers, hence the subsequent weak learners will be decision tree regressors\n",
    "6. the nodes will hence store samples and their associated \n",
    "    1. **previous** learner's pseudoresiduals, **including the leafs**\n",
    "    2. **previous** learner's **probabilities**, **including the leafs**\n",
    "7. the log odds for this learner, lets call it $\\Gamma_1$, is defined for each leaf node:\n",
    "    1. $\\Gamma_1 = \\dfrac{\\sum\\limits_{\\textrm{samples in this node}} \\textrm{sum of previous learner's residuals} }{\\sum\\limits_{\\textrm{samples in this node}} \\textrm{previous learner's probability}\\times(1-\\textrm{previous learner's probability})}$\n",
    "    2. Checkout [this blogpost](https://towardsdatascience.com/all-you-need-to-know-about-gradient-boosting-algorithm-part-2-classification-d3ed8f56541e) to know the derivation of the formula below:\n",
    "8. At this stage, $F(x) = F_0(x) + \\eta \\times \\Gamma_1$ where $\\Gamma_1$ of that leaf node is used where the sample being predicted ends up in (as per the weak learner's decision path)\n",
    "    1. The actual predictions at this stage: $p(x) = \\dfrac{1}{1+e^{-F(x)}}$\n",
    "9. This process is repeated iteratively till convergence.\n",
    "10. <img src=\"gradientBoostingClassifier_demo1.png\" /> <img src=\"gradientBoostingClassifier_demo2.png\" />\n",
    "11. sklearn's implementation handles the multi-class aspect which isn't addressed in the above theoretical explanation by creating as many regression trees as there as classes\n",
    "    > In each stage n_classes_ regression trees are fit on the negative gradient of the loss function, e.g. binary or multiclass log loss. Binary classification is a special case where only a single regression tree is induced.\\\n",
    "    HistGradientBoostingClassifier is a much faster variant of this algorithm for intermediate and large datasets (n_samples >= 10_000) and supports monotonic constraints.\n",
    "\n",
    "\n",
    "\n",
    "## Intuition\n",
    "1. Residuals simply denote the probability *left to compute*.\n",
    "2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disadvantages\n",
    "- memory complexity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
