{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "\testimators = [('lr', log_clf), ('rf', rnd_clf), ('svc',svm_clf)],\n",
    "\tvoting = 'hard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging\n",
    "1. boostrapped aggregation\n",
    "\n",
    "## Bootstrapping\n",
    "\n",
    "1. First, we create random samples of the training data set with replacment (we *aren't creating new samples*, we are rather bucketing samples from the main training dataset into smaller subsets).\n",
    "    \n",
    "    1. the likelihood of the models in the ensemble being exposed to truly distinct sets depends on sampling with replacement or without replacement\n",
    "    2. with replacement([`bootstrap=True`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)) shall be typically used when the dataset size is small enough s.t. not all base estimators will get at least K-sized subset.\n",
    "    3. without replacement shall be typically used when the dataset size is large enough s.t. all base estimators will get at least a K-sized subset.\n",
    "    4. when **subsets** are **created without replacement**, the process is called **subsetting instead of bootstrapping**.\n",
    "\n",
    "2. Then, we build a model (classifier or Decision tree) for each sample. \n",
    "\n",
    "## Aggregation\n",
    "\n",
    "1. **Aggregation**: Finally, results of these multiple models are combined using average or majority voting.\n",
    "\n",
    "## How Bagging = Bootstrapping + Aggregation?\n",
    "\n",
    "1. As each model is exposed to a **different subset of data**\n",
    "    1. this results in diverse models being created.\n",
    "    2. diversity results from them being trained on slightly different subsets of the original dataset. \n",
    "    3. If you were to train all the base models on the exact same dataset, they’ll make the same mistakes, and their errors will be perfectly correlated. \n",
    "    4. Bagging works because each model is forced to overfit to different pieces of the data, and therefore *makes different mistakes*.\n",
    "2. Diversity is what sets bagging apart from an individual classical ML model\n",
    "    1. Bagging prefers usage of low bias high variance models as its base estimators, like decision trees.\n",
    "    2. if say a single DT was used for modelling the problem, it will fundamentally overfit.\n",
    "    3. In bagging rather, the aggregation step is what leads to an overall lower variance\n",
    "        1. since each base DT is trained with a different subset of samples, each will be a low bias high variance model.\n",
    "        2. however, for a given test sample, the noise picked by each of the base DTs will be uncorrelated as each of these DTs has *theoretically learnt different patterns*.\n",
    "    1. Thus, Bagging helps us to reduce the variance error.\n",
    "3. Combinations of multiple models decreases variance, especially in the case of unstable models, and may produce a more reliable prediction than a single model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging as seen in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    n_estimators=500,\n",
    "    max_samples=100,\n",
    "    bootstrap=True, # usage of Bagging, for pasting change this to False\n",
    "    n_jobs=2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BaggingClassifier` automatically performs **soft voting** if the classifier can calculate the probabilities for its predictions(`predict_proba()` method)\n",
    "\n",
    "Bagging is much better than Pasting\n",
    "\n",
    "* When performing Bagging on a training set, only 63% of the instances are included in the model, that means there are 37% of the instances that the classifier has not seen before. \n",
    "    * These can be used for evaluation just like Cross-Validation.\n",
    "    \n",
    "    * To use this functionality, simply add a `oob_score = True` parameter in the `BaggingClassifier` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if Bagging actually works in practice\n",
    "- check if the claim of bootstrapping, that the underlying base estimators produce uncorrelated errors is true practically:\n",
    "- The correlation matrix will give you the correlation between the residuals of each pair of base models. \n",
    "- If bagging is working as expected, these values should be low, ideally close to 0.\n",
    "- MSE of each base model will show if they’re performing similarly or if their individual biases vary. \n",
    "    - If the errors are correlated, their MSEs should be roughly the same, and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions from each base model\n",
    "base_model_preds = np.array([model.predict(X_test) for model in bagging_model.estimators_])\n",
    "\n",
    "\n",
    "###################### Regression task ######################\n",
    "# Compute residuals (errors)\n",
    "residuals = base_model_preds - y_test.reshape(-1, 1)\n",
    "\n",
    "# Calculate the correlation matrix of the residuals\n",
    "correlation_matrix = np.corrcoef(residuals)\n",
    "\n",
    "# Print the correlation matrix (it's the key here)\n",
    "print(\"Correlation matrix of residuals between base models:\\n\", correlation_matrix)\n",
    "\n",
    "# Optionally, you can also calculate the mean squared error for each base model to see performance differences\n",
    "errors = np.array([mean_squared_error(y_test, preds) for preds in base_model_preds])\n",
    "print(\"Mean Squared Errors for each base model: \", errors)\n",
    "\n",
    "###################### Classification task ######################\n",
    "# Compute misclassification residuals (1 if wrong, 0 if correct)\n",
    "misclassifications = (base_model_preds != y_test.reshape(-1, 1)).astype(int)\n",
    "\n",
    "# Calculate the correlation matrix of the misclassifications\n",
    "correlation_matrix = np.corrcoef(misclassifications)\n",
    "\n",
    "#################  AUTOMATED EVALUATION OF CORRELATION MATRIX ############################\n",
    "#################  WHEN NO. OF ESTIMATORS IS TOO HIGH ####################################\n",
    "# Find the pairwise correlations that are too high (above some threshold, e.g., 0.9)\n",
    "high_correlation_pairs = np.where(np.abs(correlation_matrix) > 0.9)\n",
    "high_correlation_pairs = [(i, j) for i, j in zip(*high_correlation_pairs) if i < j]\n",
    "\n",
    "# Output the problematic pairs of base models\n",
    "if high_correlation_pairs:\n",
    "    print(\"Found high correlation between the following base model pairs (indices):\")\n",
    "    for pair in high_correlation_pairs:\n",
    "        print(pair)\n",
    "else:\n",
    "    print(\"No significant correlation detected between the errors of the base models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random patches and random subspaces\n",
    "\n",
    "1. All ensemble techniques up until now sampled only the training instances, but kept all the features(`bootstrap_features = False`).\n",
    "\n",
    "2. patches samples both training instances and features(out of d features, k are chosen at random, just as how training instances were chosen at random)\n",
    "\n",
    "3. Random Subspaces keeps all the instances but samples features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patchedBagClf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    n_estimators=500,\n",
    "    max_samples=0.6,\n",
    "    bootstrap=True, # usage of Bagging, for pasting change this to False\n",
    "    n_jobs=2,\n",
    "    random_state=42,\n",
    "    bootstrap_features=True\n",
    ")\n",
    "\n",
    "subspaceBagClf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    n_estimators=500,\n",
    "    max_samples=0.6,\n",
    "    bootstrap=True, # usage of Bagging, for pasting change this to False\n",
    "    n_jobs=2,\n",
    "    random_state=42,\n",
    "    bootstrap_features=True,\n",
    "    max_features=0.6\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
