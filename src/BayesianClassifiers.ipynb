{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes' Theorem\n",
    "\n",
    "1. Conditional Probability\n",
    "    1. P(A|B) = $\\frac{P(A\\cap B)}{P(B)}$\n",
    "    \n",
    "    2. suppose scenario = bag with 3 black marbles and 2 red marbles\n",
    "        \n",
    "        1. first event(A): marble picked = black, second(B) picked = red\n",
    "        \n",
    "        2. P(A) = $\\frac{2}{5}$, P(B) = $\\frac{3}{5}$\n",
    "        \n",
    "        3. the catch here is that B occurs after A, hence the event actually is B|A(event B occurs **given that A has already occurred**)\n",
    "        \n",
    "        4. hence P(A $\\cap$ B) = P(A).P(B|A) = $\\frac{2}{5}\\times\\frac{3}{4}$ = $\\frac{3}{10}$\n",
    "        \n",
    "        5. theoretically, it can be interpreted as constructing the set of sample space of events: {(red, red), (red, black), (black, red), (black, black)}, and since each red is unique and each black is also unique, the space actually becomes = {(r$_1$, r$_2$), (r$_1$, r$_3$), (r$_2$, r$_1$).....(r$_1$, b$_1$), (r$_1$, b$_2$), (r$_2$, b$_1$) ....., (b$_1$, r$_1$), (b$_1$, r$_2$).... (b$_1$, b$_2$), (b$_2$, b$_1$) }\n",
    "        \n",
    "        and the event that we are looking for is (b$_i$, r$_j$)\n",
    "        \n",
    "        this event will occur 6 out of the total $5 \\choose 2$ $\\times 2$ = 20(since the ordered-pair nature of picking out the marbles also corresponds to different events taking place) , which evaluates to $\\frac{3}{10}$\n",
    "        \n",
    "    3. hence we have P(A|B) = $\\frac{P(A\\cap B)}{P(B)}$ and P(B|A) = $\\frac{P(A\\cap B)}{P(A)}$\n",
    "    \n",
    "    4. thus $P(A|B).P(B) = P(B|A).P(A)$ , this is the **Bayes theorem**\n",
    "    \n",
    "2. the Bayes' theorem is actually structured as \\\n",
    "P(A|B) = $\\frac{P(B|A).P(A)}{P(B)}$ , where P(A|B) = posterior probability, **P(B|A) = likelihood**, <font color=\"red\">P(A) = apriori probability</font>, P(B) = marginal.\n",
    "\n",
    "    1. its because we already know everything about B, and we know that when A occurs in isolation, what happens.\n",
    "    \n",
    "    2. what we don't know is that what happens when A occurs after B, hence simply the occurrence of A is termed **apriori probability**\n",
    "    \n",
    "    3. here B was the independent event - happened on its own without the **triggering** of another event, whereas A required B to happen first, thus making **A the dependent event on B**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier\n",
    "\n",
    "1. as we know about dependent and independent events, we have features x$_1$, x$_2$.....x$_d$ which are all independent variables , and a quantity to be predicted - y which we refer to as the dependent variable, the analogy to events and variables should be pretty clear!!!.\n",
    "\n",
    "2. hence P(y|x$_1$, x$_2$....x$_d$) = <font size=\"4\">$\\frac{P(x_1|y).P(x_2|y).P(x_3|y)...P(x_d|y).P(y)}{P(x_1).P( x_2)....P(x_d)}$</font>\n",
    "    1. this means that before occuring y, events x$_1$ to x$_d$ have already occurred\n",
    "    \n",
    "    2. P(x$_2$|x$_1$) = $\\frac{P(x_1|x_2).P(x_2)}{P(x_1)}$, P(x$_3$|x$_2$ && x$_1$) = $\\frac{P(x_3\\cap(x_1\\cap x_2))}{P(x_1\\cap x_2)}$ = \n",
    "    \n",
    "3. hence the posteriori for classification problem is directly proportional to the product of apriori and product of all likelihoods.\n",
    "\n",
    "4. we need to find the y value which maximizes this posteriori, since that will be the **most likely** event to occur, or the **most likely class that the object belongs to**.\n",
    "\n",
    "5. hence, maximize P(y).$\\prod\\limits_{i=1}^{d}P(x_i|y)$ = objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application in NLP\n",
    "\n",
    "1. generate vector-encoding for each of the words\n",
    "\n",
    "2. usually used in text-classification tasks such as mood/sentiment analysis\n",
    "\n",
    "3. usually count-vectorized/tfidf-vectorized vectors are used as feature-representation of each word(sample)\n",
    "\n",
    "4. P(y = 1 | sentences-set) = ? , sentences = a pre-defined sequence of words, i.e. a sequence of vectors\n",
    "\n",
    "    1. here the term P(x$_i$|y), for instance w.r.t. the problem of sentiment classification, would mean occurence probability of a word, given that the conveyed-information is positive(y=1) or negative sentiment(y=0).\n",
    "    \n",
    "    2. each x$_i$ obviously represents a unique word.\n",
    "    \n",
    "5. here, apriori probability can be simple learnt from the output label-y from the *training-data*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_cpu] *",
   "language": "python",
   "name": "conda-env-tensorflow_cpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
