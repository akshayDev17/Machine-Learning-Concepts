{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes' Theorem\n",
    "\n",
    "1. Conditional Probability\n",
    "    1. P(A|B) = $\\frac{P(A\\cap B)}{P(B)}$\n",
    "    \n",
    "    2. suppose scenario = bag with 3 black marbles and 2 red marbles\n",
    "        \n",
    "        1. first event(A): marble picked = black, second(B) picked = red\n",
    "        \n",
    "        2. P(A) = $\\frac{2}{5}$, P(B) = $\\frac{3}{5}$\n",
    "        \n",
    "        3. the catch here is that B occurs after A, hence the event actually is B|A(event B occurs **given that A has already occurred**)\n",
    "        \n",
    "        4. hence P(A $\\cap$ B) = P(A).P(B|A) = $\\frac{2}{5}\\times\\frac{3}{4}$ = $\\frac{3}{10}$\n",
    "        \n",
    "        5. theoretically, it can be interpreted as constructing the set of sample space of events: {(red, red), (red, black), (black, red), (black, black)}, and since each red is unique and each black is also unique, the space actually becomes = {($r_1$, $r_2$), ($r_1$, $r_3$), ($r_2$, $r_1$).....($r_1$, $b_1$), ($r_1$, $b_2$), ($r_2$, $b_1$) ....., ($b_1$, $r_1$), ($b_1$, $r_2$).... ($b_1$, $b_2$), ($b_2$, $b_1$) }\n",
    "        \n",
    "        and the event that we are looking for is ($b_i$, $r_j$)\n",
    "        \n",
    "        this event will occur 6 out of the total $5 \\choose 2$ $\\times 2$ = 20(since the ordered-pair nature of picking out the marbles also corresponds to different events taking place) , which evaluates to $\\frac{3}{10}$\n",
    "        \n",
    "    3. hence we have P(A|B) = $\\frac{P(A\\cap B)}{P(B)}$ and P(B|A) = $\\frac{P(A\\cap B)}{P(A)}$\n",
    "    \n",
    "    4. thus $P(A|B).P(B) = P(B|A).P(A)$ , this is the **Bayes theorem**\n",
    "    \n",
    "2. the Bayes' theorem is actually structured as \\\n",
    "P(A|B) = $\\frac{P(B|A).P(A)}{P(B)}$ , where P(A|B) = posterior probability, **P(B|A) = likelihood**, <font color=\"red\">P(A) = apriori probability</font>, P(B) = marginal.\n",
    "\n",
    "    1. its because we already know everything about B, and we know that when A occurs in isolation, what happens.\n",
    "    \n",
    "    2. what we don't know is that what happens when A occurs after B, hence simply the occurrence of A is termed **apriori probability**\n",
    "    \n",
    "    3. here B was the independent event - happened on its own without the **triggering** of another event, whereas A required B to happen first, thus making **A the dependent event on B**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier\n",
    "\n",
    "1. before assuming anything, a prediction problem can be framed as given a certain set of features ($x_1\\cdots x_d$), predict $y$\n",
    "    - corresponding to estimate $P(y \\, |\\, x_1 \\,,\\, x_2 \\cdots x_d)$\n",
    "    - which, using Bayes' Theorem, can be written as $ \\dfrac{P(x_1 \\,,\\, x_2 \\cdots x_d | y).P(y)}{P(x_1 \\,,\\, x_2 \\cdots x_d)}$\n",
    "        - for 2 events, bayes theorem: $P(y|x_1) = \\dfrac{P(y,x_1)}{P(x_1)} \\rightarrow P(y,x_1) = P(y|x_1).P(x_1) = P(x_1|y).P(y) \\Rightarrow \\quad P(y|x_1) = \\dfrac{P(x_1|y).P(y)}{P(x_1)}$\n",
    "        - for 3 events, bayes theorem: $P(y |x_1, x_2) = \\dfrac{P(y,x_1,x_2)}{P(x_1, x_2)} \\rightarrow P(y,x_1,x_2) = P( (A),(C|B)).P(B) = P(A|C|B).P(C|B).P(B) = P(A|B,C).P(C|B).P(B)$\n",
    "    - using this *chain rule of bayesian probability*, $P(y \\, |\\, x_1 \\,,\\, x_2 \\cdots x_d) = \\dfrac{P(x_1 \\,,\\, x_2 \\cdots x_d | y).P(y)}{P(x_1 \\,,\\, x_2 \\cdots x_d)} = \\dfrac{P(x_1|y).P(x_2|x_1, y).P(x_3|x_1,x_2,y)\\cdots P(x_d|x_1,x_2\\cdots x_{d-1},y)}{P(x_1 \\,,\\, x_2 \\cdots x_d)}$\n",
    "\n",
    "1. as we know about dependent and independent events, we have features $x_1$, $x_2$.....$x_d$ which are all independent variables , and a quantity to be predicted - y which we refer to as the dependent variable, the analogy to events and variables should be pretty clear!!!.\n",
    "    - independent means that $P(A,B) = P(A).P(B) \\Rightarrow P(B|A) = P(B) \\,,\\, P(A|B) = P(A)$\n",
    "\n",
    "2. hence $P(y \\, |\\, x_1 \\,,\\, x_2 \\cdots x_d) = \\dfrac{P(x_1|y).P(x_2|y).P(x_3|y)...P(x_d|y).P(y)}{P(x_1).P( x_2)....P(x_d)}$\n",
    "    1. this means that before occuring y, events $x_1$ to $x_d$ have already occurred\n",
    "    \n",
    "3. hence the posteriori for classification problem is directly proportional to the product of apriori and product of all likelihoods (likelihood of that feature given that class value).\n",
    "\n",
    "4. we need to find the y value which maximizes this posteriori, since that will be the **most likely** event to occur, or the **most likely class that the object belongs to**.\n",
    "\n",
    "5. hence, maximize P(y).$\\prod\\limits_{i=1}^{d}P(x_i|y)$ = objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application in NLP\n",
    "\n",
    "1. generate vector-encoding for each of the words\n",
    "\n",
    "2. usually used in text-classification tasks such as mood/sentiment analysis\n",
    "\n",
    "3. usually count-vectorized/tfidf-vectorized vectors are used as feature-representation of each word(sample)\n",
    "\n",
    "4. P(y = 1 | sentences-set) = ? , sentences = a pre-defined sequence of words, i.e. a sequence of vectors\n",
    "\n",
    "    1. here the term $P(x_i|y)$, for instance w.r.t. the problem of sentiment classification, would mean occurence probability of a word, given that the conveyed-information is positive(y=1) or negative sentiment(y=0).\n",
    "    \n",
    "    2. each $x_i$ obviously represents a unique word.\n",
    "    \n",
    "5. here, apriori probability can be simple learnt from the output label-y from the *training-data*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_cpu] *",
   "language": "python",
   "name": "conda-env-tensorflow_cpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
